
95-702 Distributed Systems  Due Monday, April 22, 2019 at 11:59:59 PM


Project 5                   Programming with Map Reduce and Spark

Learning objectives: The student will be able to program MapReduce
applications and Spark applications.

This file is in plain text so that you may easily copy and paste
unix statements and program code.

The purpose of this project is to introduce the student to big data,
Hadoop (HDFS), and MapReduce. We will be running MapReduce on top of
Hadoop. For those new to Linux, this project will also expose you to
compiling and running Java programs on Linux.

We will be working with two different directory structures. The first is
your local directory structure /home/userID. The second is the HDFS
directory structure /user/userID. We will compile code on the first
(/home/userID) and then deploy jar files to the second (/user/userID).
The input to the program must be stored on HDFS and the output will be
generated to HDFS. So, we will be copying input from our local system
to HDFS before running a jar. We will be copying the output from HDFS
to our local directory with the hadoop getMerge utility.

The code and example data for the MaxTemperature application is from
"Hadoop: The Definitive Guide, Second Edition, by Tom White.
Copyright 2011 Tom White, 978-1-449-38973-4."

We will be running Linux, Centos 6.3, on a Hadoop cluster of several
physical machines on a single rack at Heinz College.

To reach the hadoop cluster, you will need to ssh into
heinz-jumbo.heinz.cmu.local.

If you want to work from home, first run the Cisco AnyConnect
Secure Mobility Client. This tool may be downloaded from here:

https://www.cmu.edu/computing/software/all/cisco-anyconnect/index.html

There are linux text editors available (pico or nano). There is also vi.
You should spend a little time learning your text editor of choice. There
is plenty of help online.

Some useful vi commands are:

To start vi, enter vi filename.
To quit vi without saving esc : q!
To quit vi with saving esc: wq
To save the current file without quitting esc : w

To move the curser down use j.
To move the curser up use k.
To move the curser left use h.
To move the curser right use l.

To append after the curser use a.
To insert before the curser use i.
To exit insert mode use esc :.
To perform a global replacement
in vi, use esc : %s/xyz/abc/g.

Your home directory is /home/userID.

The hadoop jars and binaries are at /usr/local/hadoop.
Your HDFS file system directories are at /user/userID/output.

Note that one is "usr" and the other is "user".

It is assumed that students with Windows machines will be running putty.
It is also assumed that each student has a user-ID and password for jumbo.

You should have established a new password when you completed the
Hadoop lab. Your user-ID should be on the grade book on Canvas. You should
communicate your password to the TA's by taking the quiz which asks for your
password.

If you need to transfer data from your machine to jumbo, be sure to use
sftp. The "copy and paste" approach may introduce hidden characters in your
file. You will need to sftp your file to jumbo. Here is an example execution
of sftp.

$sftp userID@heinz-jumbo.heinz.cmu.local
sftp>put MaxTemperature.java
sftp>get MaxTemperature.java


Here are some basic commands that you will need to study before attempting to work on the cluster.
They are meant for reference. When working on the tasks below, you will need to use the commands
here (but with appropriate modifications based on the task.) These commands will not run unless
they are executed in the appropriate order and with appropriate resources. Spend some time
studying them and then begin to use those that you need to complete the tasks.

Use the up arrow in unix to select previously executed commands.
====================================================================================

The user would like to see a list of files on the Hadoop distributed file system under /user/userID/input.

Note again the use of "user" and not "usr". "user" is a reference to the directory in HDFS. "usr" is a reference
to your local directory.

$hadoop dfs -ls /user/userID/input/

An input file needs to be placed under HDFS. This file will be used for subsequent map reduce processing.
The HDFS file and directory are created. If they already exist, then this command will return an 'already exists'
message.

$hadoop dfs -copyFromLocal /home/userID/input/1902.txt /user/userID/input/1902.txt

Look at the contents of a file on HDFS.

hadoop dfs -cat /user/userID/input/testFile

Remove a local directory of Java classes that may contain Java packages.

$rm -r temperature_classes

Create a directory for Java classes.

$mkdir temperature_classes

Compile three Java classes using a library of Hadoop classes stored in
a jar. The classes directory (./temperature_classes) is also consulted
during the compile. The classes directory (./temperature_classes) is the
target of the compile and may be populated with a a directory structure
that corresponds to Java packages. The Java files are in the current
directory. The directory temperature_classes exists as a subdirectory of
the current directory.

These commands assume that you are in a directory just above temperature_classes.
So, before running the first compile, if we execute an 'ls' command, we would see:

$ls
temperature_classes MaxTemperatureMapper.java MaxTemperatureReducer.java MaxTemperature.java

$javac -classpath  /usr/local/hadoop/hadoop-core-1.2.1.jar:./temperature_classes -d temperature_classes MaxTemperatureMapper.java

$javac -classpath  /usr/local/hadoop/hadoop-core-1.2.1.jar:./temperature_classes -d temperature_classes MaxTemperatureReducer.java

$javac -classpath  /usr/local/hadoop/hadoop-core-1.2.1.jar:./temperature_classes -d temperature_classes MaxTemperature.java

Remove an existing jar file.

$rm temperature.jar

Create a new jar file called temperature.jar. It will include all of the classes found in temperature_classes. Note the "." at the end of the line. The dot is important here. It refers
to the current directory. Note too that temperature_classes must be a subdirectory of the
current directory.

$jar -cvf temperature.jar -C  temperature_classes/  .

Remove the output directory from the distributed file system.

$hadoop dfs -rmr /user/userID/output

Merge and copy files from the Hadoop Distributed File system to the client.
hadoop dfs -getmerge /user/userID/output aCoolLocalFile

You may view what jobs are running on the cluster with the command:
hadoop job -list

Kill a job that is not making progress:
bin/hadoop job -kill job_201310251241_0754
You will need your own Job ID.

Execute a map reduce job on the cluster of machines. The file temperature.jar holds the map reduce code. Next,
a path to the class with a main routine is provided. Then, the input and output is specified. The input is a file
that must exist on the distributed file system and the output is a directory that will be created. It must
not exist before running the command or you will receive an exception.

$ hadoop jar /home/userID/temperature.jar edu.cmu.andrew.mm6.MaxTemperature  /user/userID/input/combinedYears.txt /user/userID/output

We wish to get a copy of the content on the output directory.

$mkdir coolProjectOutput

$hadoop dfs -getmerge /user/userID/output ~/coolProjectOutput/

$cat ~/coolProjectOutput/output


Note that the code below is defined within Java packages. So, when you compile
the source code the compiled (.class files) will be placed within directories
and sub directories.

=============================================================================

Proper submission of Project5 is worth 5% of the total Project 5 grade. In
order to submit properly, your directory structure must be as described below.

Make a directory in your home directory called Project5.

cd
mkdir Project5

Within the Project5 directory, make additional subdirectories with the
names Task0, Task1, Task2 and so on. Place all Task0 work within the
Task0 directory. Place all Task1 work within the Task1 directory and so on.

For Task0,

cd Project5
mkdir Task0
cd Task0


Task 0   (5 Points. Graded only on correct execution)
=======

Compile and execute a MapReduce job developed from WordCount.java. The file
WordCount.java is found under /home/public/. The code is also available at
the bottom of this document. Copy it to your home directory. Compile it and
generate a jar file called wordcount.jar. Deploy the jar file and test it
against /home/public/words.txt. The file words.txt will need to be copied
to HDFS using Hadoop's copyFromLocal command.

Note, you have more than one output file on the cluster. Each output file is
generated by a reducer.

Examine the last line of each output file with
hadoop dfs -cat /user/userID/output/part-r-00000
hadoop dfs -cat /user/userID/output/part-r-00001
hadoop dfs -cat /user/userID/output/part-r-00002

The final output should be merged and left in
your /home/userID/Project5/Part_1/Task0/Task0Output file.


Task 1  (8 Points. Graded only on correct execution)
======

Working from WordCount.java, build and deploy a MapReduce application
called TotalWords.java and place it into a jar file called total_words.jar.
This program will compute the total number of words in words.txt. The output
should be left in your /home/userID/Project5/Part_1/Task1/Task1Output file. Note that
WordCount.java (Task 0) uses a call to nextToken() on the iterator. Without this call,
the program will enter an infinite loop and will need to be killed.

Task 2  (8 Points. Graded on correct execution 6 points and solid documentation 2 points)
======

Modify the code in Task 1 so that it reads the words.txt file and computes
the number of words in the file that contain the substring 'rr'. That is,
we are counting words that contain the two lower case letters 'r' and 'r' where
'r' is immediately followed by another 'r'.

Call this application RandR.java and place it in the jar file called
randr.jar. The output should be left in your
/home/userID/Project5/Part_1/Task2/Task2Output file.

This Task requires that you document your code. The grading of this task will include a careful
look at the documentation - your own words describing the behavior of this MapReduce application.

Task 3  (8 Points. Graded only on correct execution.)
======

Modify the code in Task 2 so that it also displays the number of words that do
not contain 'rr'. That is, it does everything that
Task 2 does but has an additional output. The total from Task 1 should be equal
to the sum of the two totals from this task. Call this program RRandNotRR.java.
Place it in a jar file called rrandnotrr.jar

The output should be left in your /home/userID/Project5/Part_1/Task3/Task3Output file.

Task 4 (8 Points. Graded only on correct execution)
======

In this task, we will use a fairly large dataset from Tom White's book.
The data file, combinedYears.txt, contains thousands of temperature readings
(in Celsius times 10) in the USA by year from 1901 to 1902. The year is specified
in positions 15-18. The temperature is specified beginning at column 87 and begins
with a plus or minus character. Four digits are used for the temperature. Be sure
to study the code below to see how it references these locations.

Below are three files: MaxTemperature.java, MaxTemperatureMapper.java and MaxTemperatureReducer.java.
These files may be found at /home/public. Copy these files to your
/home/userID/Project5/Part_1/Task4 directory.

Run this application against the data set under /home/public/combinedYears.txt. Your jar file will
be named maxtemperature.jar.  The output should be left in your
/home/userID/Project5/Part_1/Task4/Task4Output file.

Task 5 (8 Points. Graded only on correct execution)
======

Modify the code from Task 4 and build a min temperature application. Note that the
temperatures in this file are degrees Celsius * 10.  Your jar file will be named
mintemperature.jar.  The output should be left in your
/home/userID/Project5/Part_1/Task5/Task5Output
file.

Task 6 (16 Points. Graded on correct execution 12 points and solid documentation 4 points)
======

Within the /home/public directory, there is a file called P1V.txt.

P1V.txt is a tab delimited text file with a individual criminal offense incidents
from January 1990 through December 1999 for serious violent crimes (FBI Part 1) in Pittsburgh.

The first two columns (X,Y) represent State Plane (projected, rectilinear) coordinates (measured
in feet) specifying the location of the crime.
The third column is the time.
The fourth column is a street address.
The fifth column is the type of offense (aggravated assault, Robbery, Rape, Etc.)
The sixth column is the date.
The seventh column is the 2000 census tract.

Write a MapReduce application that finds the total number of rapes and
robberies. If there were 100 rapes and 50 robberies then this program
would display 150 (100 + 50).

Your jar file will be named rapesplusrobberies.jar. The output should be left
in your /home/userID/Project5/Part_1/Task6/Task6Output file.

This Task requires that you document your code. The grading of this task will include a careful
look at the documentation - your own words describing the behavior of this MapReduce application.


Task 7 (8 Points. Graded on correct execution 5 points and solid documentation 3 points)
======

Modify your solution to Task 6 so that it finds the total number of aggravated assault crimes that occurred within
200 meters of 3803 Forbes Avenue in Oakland. This location has the (X,Y) coordinates of
(1354326.897,411447.7828). Use these coordinates and the Pythagorean theorem to decide if a particular
aggravated assault occurred within 200 meters of 3803 Forbes Avenue. State plane coordinates are measured in feet.
Your code is testing on meters.

Your jar file will be named oaklandcrimestats.jar. The output should be left
in your /home/userID/Project5/Part_1/Task7/Task7Output file.

This Task requires that you document your code. The grading of this task will include a careful
look at the documentation - your own words describing the behavior of this MapReduce application.

Task 8 (8 Points. Graded on correct execution 5 points and correct screenshot 3 points)
======

In Task 8, the input file is named CrimeLatLonXYTabs.txt. It can be copied from the /home/public directory.
Its format is similar to P1V.txt but also includes the latitude and longitude of each crime.

CrimeLatLonXYTabs.txt is a tab delimited text file with a individual criminal offense incidents
from January 1990 through December 1999 for serious violent crimes (FBI Part 1) in Pittsburgh.

The first two columns (X,Y) represent State Plane (projected, rectilinear) coordinates (measured
in feet) specifying the location of the crime.
The third column is the time.
The fourth column is a street address.
The fifth column is the type of offense (aggravated assault, rape, Etc.)
The sixth column is the date.
The seventh column is the 2000 census tract.
The eight column specifies the latitude.
The ninth column specifies the longitude.

These last two columns are used for viewing in GIS tools (such as Google Earth Pro).

As in Task 7, modify your solution to Task 6 so that it finds the total number of aggravated assault crimes that occurred within
200 meters of 3803 Forbes Avenue in Oakland. This location has the (X,Y) coordinates of
(1354326.897,411447.7828). Use these coordinates and the Pythagorean theorem to decide if a particular
aggravated assault occurred within 200 meters of 3803 Forbes Avenue. State plane coordinates are measured in feet.
Your code is testing on meters.

Your jar file will be named oaklandcrimestatskml.jar. The output file will be a well formed KML file
suitable for viewing in Google Earth. The output file will be left
in your /home/userID/Project5/Part_1/Task8/Task8Output file. You will also need to
provide a screenshot showing Google Earth viewing the KML. Include the screenshot
in /home/userID/Project5/Part_1/Task8/Task8OutputGoogleEarthScreenshot

You should consider limiting the number of reducers to 1. You can do this by
generating the same key from each mapper.

This Task requires that you document your code. The grading of this task will include a careful
look at the documentation - your own words describing the behavior of this MapReduce application.


Part 1 Summary
==============

For Tasks 2, 6, and 7, the grader will be looking for clear documentation. You do not
need to use Javadoc but plenty of comments will clearly describe what is going on in your
code. Include your name as the author and describe what the code is doing.

After submission, leave your Hadoop account alone. The TA's may need to visit your
account and test the code there.


// ======================= WordCount.java ==========================================
package org.myorg;
import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.*;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.util.*;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;


public class WordCount extends Configured implements Tool {

        public static class WordCountMap extends Mapper<LongWritable, Text, Text, IntWritable>
        {
                private final static IntWritable one = new IntWritable(1);
                private Text word = new Text();

                @Override
                public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException
                {
                        String line = value.toString();
                        StringTokenizer tokenizer = new StringTokenizer(line);
                        while(tokenizer.hasMoreTokens())
                        {
                                word.set(tokenizer.nextToken());
                                context.write(word, one);
                        }
                }
        }

        public static class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable>
        {
                public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException
                {
                        int sum = 0;
                        for(IntWritable value: values)
                        {
                                sum += value.get();
                        }
                        context.write(key, new IntWritable(sum));
                }

        }

        public int run(String[] args) throws Exception  {

                Job job = new Job(getConf());
                job.setJarByClass(WordCount.class);
                job.setJobName("wordcount");

                job.setOutputKeyClass(Text.class);
                job.setOutputValueClass(IntWritable.class);

                job.setMapperClass(WordCountMap.class);
                job.setReducerClass(WordCountReducer.class);


                job.setInputFormatClass(TextInputFormat.class);
                job.setOutputFormatClass(TextOutputFormat.class);


                FileInputFormat.setInputPaths(job, new Path(args[0]));
                FileOutputFormat.setOutputPath(job, new Path(args[1]));

                boolean success = job.waitForCompletion(true);
                return success ? 0: 1;
        }


        public static void main(String[] args) throws Exception {
                // TODO Auto-generated method stub
                int result = ToolRunner.run(new WordCount(), args);
                System.exit(result);
        }

}


// ============== MaxTemperature.java ================================
        package edu.cmu.andrew.mm6;
        import java.io.IOException;
	import org.apache.hadoop.io.IntWritable;
	import org.apache.hadoop.io.LongWritable;
	import org.apache.hadoop.io.Text;
	import org.apache.hadoop.mapred.MapReduceBase;
	import org.apache.hadoop.mapred.Mapper;
	import org.apache.hadoop.mapred.OutputCollector;
	import org.apache.hadoop.mapred.Reporter;

	public class MaxTemperatureMapper extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {
		private static final int MISSING = 9999;
		public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws 			IOException {

                        // Get line from input file. This was passed in by Hadoop as value.
                        // We have no use for the key (file offset) so we are ignoring it.

			String line = value.toString();

                        // Get year when weather data was collected. The year is in positions 15-18.
                        // This field is at a fixed position within a line.
			String year = line.substring(15, 19);

                        // Get the temperature too.

			int airTemperature;
			if (line.charAt(87) == '+') { // parseInt doesn't like leading plus signs
				airTemperature = Integer.parseInt(line.substring(88, 92));
			} else {
				airTemperature = Integer.parseInt(line.substring(87, 92));
                        }

                        // Get quality of reading. If not missing and of good quality then
                        // produce intermediate (year,temp).

                        String quality = line.substring(92, 93);
			if (airTemperature != MISSING && quality.matches("[01459]")) {

                             // for each year in input, reduce will be called with
                             // (year,[temp,temp,temp, ...])
                             // They key is year and the list of temps will be placed in an iterator.

				output.collect(new Text(year), new IntWritable(airTemperature)); }
			}
	 }


=========== MaxTemperatureReducer.java ====================================================
	package edu.cmu.andrew.mm6;
        import java.io.IOException; import java.util.Iterator;
	import org.apache.hadoop.io.IntWritable;
	import org.apache.hadoop.io.Text;
	import org.apache.hadoop.mapred.MapReduceBase;
	import org.apache.hadoop.mapred.OutputCollector;
	import org.apache.hadoop.mapred.Reducer; import org.apache.hadoop.mapred.Reporter;

	public class MaxTemperatureReducer extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {

		public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text,
                                   IntWritable> output, Reporter reporter) throws IOException {

                        // from the list of values, find the maximum
                        int maxValue = Integer.MIN_VALUE;
                        while (values.hasNext()) {
			     maxValue = Math.max(maxValue, values.next().get());
                        }
                        // emit (key = year, value = maxTemp = max for year)
			output.collect(key, new IntWritable(maxValue));
		}
	}

// ======= And, to get it all running and tied together: MaxTemperature.java ============

        package edu.cmu.andrew.mm6;
	import org.apache.hadoop.fs.Path;
	import org.apache.hadoop.io.IntWritable;
	import org.apache.hadoop.io.Text;
	import org.apache.hadoop.mapred.FileInputFormat;
	import org.apache.hadoop.mapred.FileOutputFormat; import org.apache.hadoop.mapred.JobClient;
	import org.apache.hadoop.mapred.JobConf;

	public class MaxTemperature {
		public static void main(String[] args) throws IOException {
			if (args.length != 2) {
				System.err.println("Usage: MaxTemperature <input path> <output path>");
				System.exit(-1);
 			}
			JobConf conf = new JobConf(MaxTemperature.class);
			conf.setJobName("Max temperature");
			FileInputFormat.addInputPath(conf, new Path(args[0]));
			FileOutputFormat.setOutputPath(conf, new Path(args[1]));
			conf.setMapperClass(MaxTemperatureMapper.class);
			conf.setReducerClass(MaxTemperatureReducer.class);
			conf.setOutputKeyClass(Text.class);
			conf.setOutputValueClass(IntWritable.class);
			JobClient.runJob(conf);
		 }
	}

Part 2 Spark on your local machine
==================================

In Part 2 of Project 5, we will be using a data file found on the course schedule. This
data file is "The Tempest" by William Shakespeare. The file is found here:

http://www.andrew.cmu.edu/course/95-702/homework/data/SparkDataFiles/TheTempest.txt

You need to download this file to your local machine and set your working directory
and file name in Netbeans.

Part 2 Submission Notes
=======================

In Part 2, you will submit one Java Maven project named Project5/Part_2/Project5Spark. It
will contain only one Java source file named TempestAnalytics.java. TempestAnalytics.java
will contain logic for each of the following tasks. In other words, you do not need
separate directories for these tasks. Simply add each task to the code in the file
TempestAnalytics.java. That one file will contain all of the functionality listed
here as tasks.

Task 0. (3  points for execution and documentation)
=======
Using the count method of the JavaRDD class, display the number of lines in "The Tempest".
For the display, use System.out.println().


Task 1. (3 points for execution and documentation)
=======
Using the split method of the java String class and the flatMap method of the JavaRDD class,
use the count method of the JavaRDD class to display the number of words in The Tempest.
For the display, use System.out.println().


Task 2. (3 points for execution and documentation)
=======
Using some of the work you did above and the JavaRDD distinct() and count() methods, display
the number of distinct words in The Tempest. For the display, use System.out.println().

Task 3. (3 points for execution and documentation)
=======
Using the JavaPairRDD class and the saveAsTextFile() method along with the JavaRDD class
and the mapToPair() method, show each word paired with the digit 1 in the output directory
named Project5/Part_2/TheTempestOutputDir1. You may re-use RDD's from the above tasks if
appropriate. Here, we are not using System.out.println().


Task 4. (3 points for execution and documentation)
=======
Using work from above and the JavaPairRDD from Task 3, create a new JavaPairRDD with the
reduceByKey() method. Save the RDD using the saveAsTextFile() method and place the result in
the output directory named Project5/Part_2/TheTempestOutputDir2. Here, we are not using
System.out.println().In my solution, one line of output is (magic,3).


Task 5. (3 points for execution and documentation)
=======
Using work from above and the JavaRDD foreach() method, prompt the user for a string and
then perform a search on every line of the The Tempest. If any line of The Tempest
contains the String entered by the user then display the entire line. For the display,
use System.out.println().


Project 5 Submission Notes
==========================

We need to submit a single zip file to Canvas. The zip file will
contain Part_1 and Part_2 subdirectories.

On HDFS, zip all of the contents of the Project5/Part_1 directory.
zip -r YourAndrewID.zip Part_1

Use sftp to copy the file YourAndrewID.zip to your laptop.

Your Project5 directory on your laptop will contain YourAndrewID.zip
and the Part_2 subdirectory.

Zip your laptop's Project5 directory:
zip -r YourAndrewIDProject5.zip Project5

Copy YourAndrewIDProject5.zip to Canvas in the slot for Project5.
